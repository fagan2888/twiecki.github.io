<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>While My MCMC Gently Samples</title><link href="https://twiecki.io/" rel="alternate"></link><link href="https://twiecki.io/feeds/all.atom.xml" rel="self"></link><id>https://twiecki.io/</id><updated>2019-03-15T10:00:00-04:00</updated><subtitle>Bayesian modeling, Data Science, and Python</subtitle><entry><title>Computational Psychiatry: Combining multiple levels of analysis to understand brain disorders - PhD thesis</title><link href="https://twiecki.io/blog/2019/03/15/computational_psychiatry_thesis/" rel="alternate"></link><published>2019-03-15T10:00:00-04:00</published><updated>2019-03-15T10:00:00-04:00</updated><author><name>Thomas Wiecki</name></author><id>tag:twiecki.io,2019-03-15:/blog/2019/03/15/computational_psychiatry_thesis/</id><summary type="html">&lt;p&gt;I noticed that as my personal website at my former university went down that my PhD thesis could not be found anywhere, so I'm posting it here.&lt;/p&gt;
&lt;p&gt;During my PhD I explored how machine learning and computational modeling of the brain can be used to improve our understanding, and diagnostics …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I noticed that as my personal website at my former university went down that my PhD thesis could not be found anywhere, so I'm posting it here.&lt;/p&gt;
&lt;p&gt;During my PhD I explored how machine learning and computational modeling of the brain can be used to improve our understanding, and diagnostics, of psychiatric illnesses. See &lt;a href="https://medium.com/@siobhankcronin/frontiers-of-computational-psychiatry-b238494265a3"&gt;this blog post by Siobhan Cronin&lt;/a&gt; for her take on the thesis.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/twiecki/WhileMyMCMCGentlySamples/raw/master/content/downloads/wiecki_phd_thesis.pdf"&gt;Download the thesis&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Abstract:
"The premise of the emerging field of computational psychiatry is to use models from computational cognitive neuroscience to gain deeper insights into mental illness. In this thesis my goal is to provide an overview of this endeavor and advance it by developing new software as well as quantitative methods. To demonstrate their use- fulness I will apply these methods to real-world data sets. A central theme will be the bridging of multiple levels of analysis of the brain ranging from neuroscience and cognition to behavior. In chapter 1 I describe the current crisis in research and treat- ment of mental illness and argue that computational psychiatry provides the tools to solve some long-standing issues that hindered progress in this area. I describe these tools by reviewing the current literature on computational psychiatry and demon- strate their usefulness on two real-world data sets. To provide a coherent scope, I will focus on response inhibition as it provides a rich literature in each of the di↵erent levels of analysis with clear links to psychopathology. In chapter 2 I first establish a neuronal basis by presenting a biologically plausible neural network model of key areas involved in response inhibition. Capturing the high-level computations of this fairly complex model requires more abstract cognitive process models. Towards this goal we developed software (chapter 3) to estimate a decision making model in a hier- archical Bayesian manner which improves parameter recovery in a simulation study. In chapter 4 I then bridge the neuronal and cognitive level by fitting a psychological process model to the simulated behavioral output of the neural network model under certain biological manipulations. By analyzing which biological manipulation is best captured by changes in certain high-level computational parameters I start to link both levels of analysis. I then apply this same psychological process model to two data sets from selective response inhibition tasks administered to patients su↵ering from Huntington’s disease (chapter 5) and depression (chapter 6). Having identified neurobiological correlates of certain model parameters allows to then formulate the- ories not only about cognitive processes impacted by these disorders but also which neuronal mechanism are likely to be involved. In addition, I demonstrate that the description of subjects’ performance by computational model parameters can lead to better classification accuracy of disease state when compared to traditionally used summary statistics."&lt;/p&gt;</content><category term="misc"></category><category term="psychiatry"></category></entry><entry><title>My foreword to "Bayesian Analysis with Python, 2nd Edition" by Osvaldo Martin</title><link href="https://twiecki.io/blog/2019/01/21/foreword_bayesian_analysis_with_python/" rel="alternate"></link><published>2019-01-21T10:00:00-05:00</published><updated>2019-01-21T10:00:00-05:00</updated><author><name>Thomas Wiecki</name></author><id>tag:twiecki.io,2019-01-21:/blog/2019/01/21/foreword_bayesian_analysis_with_python/</id><summary type="html">&lt;p&gt;When &lt;a href="https://twitter.com/aloctavodia"&gt;Osvaldo&lt;/a&gt; asked me to write the foreword to his new book I felt honored, excited, and a bit scared, so naturally I accepted. What follows is my best attempt to convey what makes probabilistic programming so exciting to me. Osvaldo did a great job with the book, it is …&lt;/p&gt;</summary><content type="html">&lt;p&gt;When &lt;a href="https://twitter.com/aloctavodia"&gt;Osvaldo&lt;/a&gt; asked me to write the foreword to his new book I felt honored, excited, and a bit scared, so naturally I accepted. What follows is my best attempt to convey what makes probabilistic programming so exciting to me. Osvaldo did a great job with the book, it is the most up-do-date resource you will find and great introduction to get into probabilistic programming, so make sure to grab a copy of &lt;a href="https://www.amazon.com/gp/product/1789341655/ref=dbs_a_def_rwt_bibl_vppi_i0"&gt;Bayesian Analysis with Python: Introduction to statistical modeling and probabilistic programming using PyMC3 and ArviZ, 2nd Edition&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you need more motivation than that, here is the foreword:&lt;/p&gt;
&lt;p&gt;"Probabilistic programming is a framework which allows you to flexibly build Bayesian statistical models in computer code. Once built, powerful inference algorithms, which work independently of the model you formulated, can be used to fit your model to data. This combination of flexible model specification and automatic inference provides a powerful tool for the researcher to quickly build, analyze and iteratively improve novel statistical models. This iterative approach is in stark contrast to the way Bayesian models were fit to data before: previous inference algorithms usually only worked for one specific model. Not only did this require strong mathematical skills to formulate the model and devise an inference scheme, it also considerably slows down the iterative cycle: change the model, rederive your inference. Probabilistic programming thus democratizes statistical modeling by considerably lowering the mathematical understanding and time required to successfully build novel models and gain unique insights into your data.&lt;/p&gt;
&lt;p&gt;The idea behind probabilistic programming is not new: BUGS, the first of its kind, was first released in 1989. The kind of models that could be fit successfully was extremely limited and inference was slow, rendering these first-generation languages not very practical. Today, there are a multitude of probabilistic programming languages which are being widely used in academia and at companies like Google, Microsoft, Amazon, Facebook and Uber to solve large and complex problems. What has changed? The key factor in lifting probabilistic programming from being a cute toy to the powerful engine that can solve complex large-scale problems is the advent of Hamiltonian Monte Carlo samplers which are several orders of magnitude more powerful than previous sampling algorithms. While originally devised in 1987, only the more recent probabilistic programming systems named Stan and PyMC3 made these samplers widely available and usable. &lt;/p&gt;
&lt;p&gt;This book will give you a practical introduction to this extremely powerful and flexible tool. It will have a deep impact in how you think about and solve complex analytical problems. There are few people better suited to have written it than PyMC3 core developer Osvaldo Martin. Osvaldo has the rare talent of breaking complex topics down to make them easily digestible. His deep practical understanding, gained through hard-won experience allow him to take you, the reader, on the most efficient route through this terrain that could otherwise easily seem impenetrable. The visualizations and code examples make this book an imminently practicable resource through which you will gain an intuitive understanding of their theoretical underpinnings.&lt;/p&gt;
&lt;p&gt;I also would like to commend you, dear reader, for having picked up this book. It is not the fast and easy route. In a time where headlines advertise deep learning as the technique to solve all current and future analytical problems, the more careful and deliberate approach of building a custom model for a specific purpose might not seem quite as attractive. However, you will be able to solve problems that can hardly be solved any other way. &lt;/p&gt;
&lt;p&gt;This is not to say that deep learning is not an extremely exciting technique. In fact, probabilistic programming itself is not constrained to classic statistical models. Reading the current machine learning literature, you will find that Bayesian statistics is emerging as a powerful framework to express and understand next-generation deep neural networks. This book will thus equip you not only with the skills to solve hard analytical problems, but also to have a front-row seat in humanities perhaps greatest endeavour: the development of artificial intelligence. Enjoy!&lt;/p&gt;
&lt;p&gt;Thomas Wiecki, PhD"&lt;/p&gt;</content><category term="misc"></category><category term="statistics"></category><category term="bayesian"></category></entry><entry><title>Using Bayesian Decision Making to Optimize Supply Chains</title><link href="https://twiecki.io/blog/2019/01/14/supply_chain/" rel="alternate"></link><published>2019-01-14T10:00:00-05:00</published><updated>2019-01-14T10:00:00-05:00</updated><author><name>Thomas Wiecki</name></author><id>tag:twiecki.io,2019-01-14:/blog/2019/01/14/supply_chain/</id><content type="html">&lt;p&gt;{% notebook supply_chain.ipynb %}&lt;/p&gt;</content><category term="misc"></category><category term="statistics"></category><category term="bayesian"></category></entry><entry><title>Hierarchical Bayesian Neural Networks with Informative Priors</title><link href="https://twiecki.io/blog/2018/08/13/hierarchical_bayesian_neural_network/" rel="alternate"></link><published>2018-08-13T10:00:00-04:00</published><updated>2018-08-13T10:00:00-04:00</updated><author><name>Thomas Wiecki</name></author><id>tag:twiecki.io,2018-08-13:/blog/2018/08/13/hierarchical_bayesian_neural_network/</id><content type="html">&lt;p&gt;{% notebook bayesian_neural_network_hierarchical.ipynb %}&lt;/p&gt;</content><category term="misc"></category><category term="statistics"></category><category term="bayesian"></category><category term="neuralnetwork"></category></entry><entry><title>An intuitive, visual guide to copulas</title><link href="https://twiecki.io/blog/2018/05/03/copulas/" rel="alternate"></link><published>2018-05-03T10:00:00-04:00</published><updated>2018-05-03T10:00:00-04:00</updated><author><name>Thomas Wiecki</name></author><id>tag:twiecki.io,2018-05-03:/blog/2018/05/03/copulas/</id><content type="html">&lt;p&gt;{% notebook copulas.ipynb %}&lt;/p&gt;</content><category term="misc"></category><category term="statistics"></category></entry><entry><title>What's new in PyMC3 3.1</title><link href="https://twiecki.io/blog/2017/07/05/new-in-pymc3-31/" rel="alternate"></link><published>2017-07-05T10:00:00-04:00</published><updated>2017-07-05T10:00:00-04:00</updated><author><name>Thomas Wiecki</name></author><id>tag:twiecki.io,2017-07-05:/blog/2017/07/05/new-in-pymc3-31/</id><content type="html">&lt;p&gt;{% notebook new_in_pymc3_3.1.ipynb %}&lt;/p&gt;</content><category term="misc"></category><category term="bayesian"></category><category term="statistics"></category><category term="pymc3"></category></entry><entry><title>Random-Walk Bayesian Deep Networks: Dealing with Non-Stationary Data</title><link href="https://twiecki.io/blog/2017/03/14/random-walk-deep-net/" rel="alternate"></link><published>2017-03-14T10:00:00-04:00</published><updated>2017-03-14T10:00:00-04:00</updated><author><name>Thomas Wiecki</name></author><id>tag:twiecki.io,2017-03-14:/blog/2017/03/14/random-walk-deep-net/</id><content type="html">&lt;p&gt;{% notebook random_walk_deep_net.ipynb %}&lt;/p&gt;</content><category term="misc"></category><category term="bayesian statistics deep learning"></category></entry><entry><title>Why hierarchical models are awesome, tricky, and Bayesian</title><link href="https://twiecki.io/blog/2017/02/08/bayesian-hierchical-non-centered/" rel="alternate"></link><published>2017-02-08T10:00:00-05:00</published><updated>2017-02-08T10:00:00-05:00</updated><author><name>Thomas Wiecki</name></author><id>tag:twiecki.io,2017-02-08:/blog/2017/02/08/bayesian-hierchical-non-centered/</id><content type="html">&lt;p&gt;{% notebook GLM_hierarchical_non_centered.ipynb %}&lt;/p&gt;</content><category term="misc"></category><category term="bayesian statistics hierarchical"></category></entry><entry><title>Bayesian Deep Learning Part II: Bridging PyMC3 and Lasagne to build a Hierarchical Neural Network</title><link href="https://twiecki.io/blog/2016/07/05/bayesian-deep-learning/" rel="alternate"></link><published>2016-07-05T10:00:00-04:00</published><updated>2016-07-05T10:00:00-04:00</updated><author><name>Thomas Wiecki</name></author><id>tag:twiecki.io,2016-07-05:/blog/2016/07/05/bayesian-deep-learning/</id><content type="html">&lt;p&gt;{% notebook bayesian_neural_network_lasagne.ipynb %}&lt;/p&gt;</content><category term="misc"></category><category term="bayesian statistics deep learning neural networks"></category></entry><entry><title>Bayesian Deep Learning Part II: Bridging PyMC3 and Lasagne to build a Hierarchical Neural Network</title><link href="https://twiecki.io/blog/2016/07/05/bayesian-deep-learning/" rel="alternate"></link><published>2016-07-05T10:00:00-04:00</published><updated>2016-07-05T10:00:00-04:00</updated><author><name>Thomas Wiecki</name></author><id>tag:twiecki.io,2016-07-05:/blog/2016/07/05/bayesian-deep-learning/</id><content type="html">&lt;p&gt;{% notebook bayesian_neural_network_lasagne.ipynb %}&lt;/p&gt;</content><category term="misc"></category><category term="bayesian statistics deep learning neural networks"></category></entry><entry><title>Bayesian Deep Learning</title><link href="https://twiecki.io/blog/2016/06/01/bayesian-deep-learning/" rel="alternate"></link><published>2016-06-01T10:00:00-04:00</published><updated>2016-06-01T10:00:00-04:00</updated><author><name>Thomas Wiecki</name></author><id>tag:twiecki.io,2016-06-01:/blog/2016/06/01/bayesian-deep-learning/</id><content type="html">&lt;p&gt;{% notebook bayesian_neural_network.ipynb %}&lt;/p&gt;</content><category term="misc"></category><category term="bayesian statistics deep learning neural networks"></category></entry><entry><title>Bayesian Deep Learning</title><link href="https://twiecki.io/blog/2016/06/01/bayesian-deep-learning/" rel="alternate"></link><published>2016-06-01T10:00:00-04:00</published><updated>2016-06-01T10:00:00-04:00</updated><author><name>Thomas Wiecki</name></author><id>tag:twiecki.io,2016-06-01:/blog/2016/06/01/bayesian-deep-learning/</id><content type="html">&lt;p&gt;{% notebook bayesian_neural_network.ipynb %}&lt;/p&gt;</content><category term="misc"></category><category term="bayesian statistics deep learning neural networks"></category></entry><entry><title>MCMC sampling for dummies</title><link href="https://twiecki.io/blog/2015/11/10/mcmc-sampling/" rel="alternate"></link><published>2015-11-10T10:00:00-05:00</published><updated>2015-11-10T10:00:00-05:00</updated><author><name>Thomas Wiecki</name></author><id>tag:twiecki.io,2015-11-10:/blog/2015/11/10/mcmc-sampling/</id><content type="html">&lt;p&gt;{% notebook MCMC-sampling-for-dummies.ipynb %}&lt;/p&gt;</content><category term="misc"></category><category term="bayesian statistics"></category></entry><entry><title>A modern guide to getting started with Data Science and Python</title><link href="https://twiecki.io/blog/2014/11/18/python-for-data-science/" rel="alternate"></link><published>2014-11-18T10:00:00-05:00</published><updated>2014-11-18T10:00:00-05:00</updated><author><name>Thomas Wiecki</name></author><id>tag:twiecki.io,2014-11-18:/blog/2014/11/18/python-for-data-science/</id><content type="html">&lt;p&gt;{% notebook Python_Data_Science.ipynb %}&lt;/p&gt;</content><category term="misc"></category><category term="intro datascience"></category></entry><entry><title>The Best Of Both Worlds: Hierarchical Linear Regression in PyMC3</title><link href="https://twiecki.io/blog/2014/03/17/bayesian-glms-3/" rel="alternate"></link><published>2014-03-17T09:00:00-04:00</published><updated>2014-03-17T09:00:00-04:00</updated><author><name>Thomas Wiecki</name></author><id>tag:twiecki.io,2014-03-17:/blog/2014/03/17/bayesian-glms-3/</id><content type="html">&lt;p&gt;{% notebook GLM_hierarchical.ipynb %}
~&lt;/p&gt;</content><category term="misc"></category><category term="bayesian statistics"></category></entry><entry><title>Easily distributing a parallel IPython Notebook on a cluster</title><link href="https://twiecki.io/blog/2014/02/24/ipython-nb-cluster/" rel="alternate"></link><published>2014-02-24T09:00:00-05:00</published><updated>2014-02-24T09:00:00-05:00</updated><author><name>Thomas Wiecki</name></author><id>tag:twiecki.io,2014-02-24:/blog/2014/02/24/ipython-nb-cluster/</id><content type="html">&lt;p&gt;{% notebook ipython_parallel_cluster.ipynb %}&lt;/p&gt;</content><category term="misc"></category><category term="computation"></category></entry><entry><title>Animating MCMC with PyMC3 and Matplotlib</title><link href="https://twiecki.io/blog/2014/01/02/visualizing-mcmc/" rel="alternate"></link><published>2014-01-02T09:00:00-05:00</published><updated>2014-01-02T09:00:00-05:00</updated><author><name>Thomas Wiecki</name></author><id>tag:twiecki.io,2014-01-02:/blog/2014/01/02/visualizing-mcmc/</id><summary type="html">&lt;p&gt;Here's the deal: I used &lt;a href="https://github.com/pymc-devs/pymc"&gt;PyMC3&lt;/a&gt;,
&lt;a href="http://matplotlib.org/"&gt;matplotlib&lt;/a&gt;, and &lt;a href="http://jakevdp.github.io/"&gt;Jake Vanderplas'&lt;/a&gt;
&lt;a href="https://github.com/jakevdp/JSAnimation"&gt;JSAnimation&lt;/a&gt; to create
javascript animations of three MCMC sampling algorithms --
&lt;a href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm"&gt;Metropolis-Hastings&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Slice_sampling"&gt;slice sampling&lt;/a&gt; and &lt;a href="http://arxiv.org/abs/1111.4246"&gt;NUTS&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I like visualizations because they provide a good intuition for how
the samplers work and what problems they can run into.&lt;/p&gt;
&lt;p&gt;You can download the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Here's the deal: I used &lt;a href="https://github.com/pymc-devs/pymc"&gt;PyMC3&lt;/a&gt;,
&lt;a href="http://matplotlib.org/"&gt;matplotlib&lt;/a&gt;, and &lt;a href="http://jakevdp.github.io/"&gt;Jake Vanderplas'&lt;/a&gt;
&lt;a href="https://github.com/jakevdp/JSAnimation"&gt;JSAnimation&lt;/a&gt; to create
javascript animations of three MCMC sampling algorithms --
&lt;a href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm"&gt;Metropolis-Hastings&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Slice_sampling"&gt;slice sampling&lt;/a&gt; and &lt;a href="http://arxiv.org/abs/1111.4246"&gt;NUTS&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I like visualizations because they provide a good intuition for how
the samplers work and what problems they can run into.&lt;/p&gt;
&lt;p&gt;You can download the full notebook &lt;a href="https://rawgithub.com/twiecki/WhileMyMCMCGentlySamples/master/content/downloads/notebooks/sample_animation.ipynb"&gt;here&lt;/a&gt; or &lt;a href="http://nbviewer.ipython.org/github/twiecki/WhileMyMCMCGentlySamples/blob/master/content/downloads/notebooks/sample_animation.ipynb?create=1"&gt;view it in your browser&lt;/a&gt;. Note that for this post I used
video embedding due to the size of the animations if they are not
compressed. The notebook contains code for both.&lt;/p&gt;
&lt;p&gt;The model is a simple linear model as explained in my &lt;a href="https://twiecki.github.io/blog/2013/08/12/bayesian-glms-1/"&gt;previous blog post on Bayesian GLMs&lt;/a&gt;. Essentially,
I generated some data and estimate &lt;code&gt;intercept&lt;/code&gt; and &lt;code&gt;slope&lt;/code&gt;. In the
lower left corner is the &lt;em&gt;joint&lt;/em&gt; posterior while the plot above shows
the &lt;em&gt;trace&lt;/em&gt; of the &lt;em&gt;marginal&lt;/em&gt; posterior of the &lt;code&gt;intercept&lt;/code&gt; while the
right plot shows the trace of the &lt;em&gt;marginal&lt;/em&gt; posterior of the &lt;code&gt;slope&lt;/code&gt; parameter. Each
point represents a sample drawn from the posterior. At 3 quarters of the way I added a thousand samples to show that they all sample from the posterior eventually.&lt;/p&gt;
&lt;h2&gt;Metropolis-Hastings&lt;/h2&gt;
&lt;p&gt;First, lets see how our old-school Metropolis-Hastings (MH)
performs. The code uses matplotlib's handy &lt;code&gt;FuncAnimation&lt;/code&gt; (see
&lt;a href="http://jakevdp.github.io/blog/2012/08/18/matplotlib-animation-tutorial/"&gt;here&lt;/a&gt;
for a tutorial), my own animation code, and the &lt;a href="https://github.com/pymc-devs/pymc/pull/433"&gt;recently merged iterative sampling function &lt;code&gt;iter_sample()&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/23zAmLruZVA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;As you can see, there is quite some correlation between &lt;code&gt;intercept&lt;/code&gt;
and &lt;code&gt;slope&lt;/code&gt; -- if we believe in a higher intercept we must also
believe in a lower slope (which makes geometrical sense if you think
how lines could fit through the point clouds). This often makes it
difficult for the MCMC algorithm to converge (i.e. sample from the
true posterior) as we wittness here.&lt;/p&gt;
&lt;p&gt;The reason MH does not do anything at first is that MH proposes huge
jumps that are not accepted because they are way outside the
posterior. PyMC then tunes the proposal distribution so that smaller
jumps are proposed. These smaller jumps however lead to the
random-walk behavior you can see which makes sampling inefficient (for
a good intuition about this "drunken walk", see
&lt;a href="http://healthyalgorithms.com/2010/03/12/a-useful-metaphor-for-explaining-mcmc/"&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;h2&gt;Slice sampling&lt;/h2&gt;
&lt;p&gt;Lets see how Slice sampling fares.&lt;/p&gt;
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/5mMwKG7rbZQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;As you can see, slice sampling does a much better job. For one thing,
there are no rejections (which is a property of the algorithm). But
there's still room for improvement. At the core, slice sampling always
updates one random variable at a time while keeping all others
constant. This property leads to small steps being taken (imagine
trying to move along a diagonal area on the chess board with a Rook)
and makes sampling from correlated posteriors inefficient.&lt;/p&gt;
&lt;h2&gt;NUTS (Hamiltonian Monte Carlo)&lt;/h2&gt;
&lt;p&gt;NUTS on the other hand is a newer gradient-based sampler that operates
on the joint posterior. Correlations are not a problem because this
sampler can actually move diagonally as well (more like the Queen). As
you can see, it does a much better job at exploring the posterior and
takes much wider steps.&lt;/p&gt;
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/Fq_hlq8AfYo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;Mesmerizing, ain't it?&lt;/p&gt;
&lt;p&gt;What surprised me about the slice sampling is that if I looked at the
individual traces (top and right plot) only, I'd say they hadn't
converged. But rather it seems that while the step-size is small,
averaging samples over a longer run should still provide meaningful
inference.&lt;/p&gt;
&lt;h2&gt;Where to go from here&lt;/h2&gt;
&lt;p&gt;I was initially setting out to get real-time plotting while sampling
into PyMC. What I've shown here just creates an animation after
sampling has finished. Unfortunately, I don't think it's currently
possible to do so in the IPython Notebook as it requires embedding of
HTML for which we need the finished product. If anyone has an idea
here that might be a very interesting extension.&lt;/p&gt;
&lt;h2&gt;Further reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://jakevdp.github.io/blog/2012/08/18/matplotlib-animation-tutorial/"&gt;Jake's tutorial on matplotlib animations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://jakevdp.github.io/blog/2013/05/19/a-javascript-viewer-for-matplotlib-animations/"&gt;Jake's blog post on embedding JS animations in the notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://healthyalgorithms.com/2011/01/28/mcmc-in-python-pymc-step-methods-and-their-pitfalls/"&gt;Abe Flaxman's much prettier videos on MCMC&lt;/a&gt;
  (Would be nice to replace my crappy plotting code with his -- PRs welcome.)&lt;/li&gt;
&lt;/ul&gt;</content><category term="misc"></category><category term="bayesian statistics"></category></entry><entry><title>Hammer time: Nailing the emcee ensemble sampler onto PyMC</title><link href="https://twiecki.io/blog/2013/09/23/emcee-pymc/" rel="alternate"></link><published>2013-09-23T09:00:00-04:00</published><updated>2013-09-23T09:00:00-04:00</updated><author><name>Thomas Wiecki</name></author><id>tag:twiecki.io,2013-09-23:/blog/2013/09/23/emcee-pymc/</id><content type="html">&lt;p&gt;{% notebook emcee.ipynb %}
~&lt;/p&gt;</content><category term="misc"></category><category term="bayesian statistics"></category></entry><entry><title>This world is far from Normal(ly distributed): Bayesian Robust Regression in PyMC3</title><link href="https://twiecki.io/blog/2013/08/27/bayesian-glms-2/" rel="alternate"></link><published>2013-08-27T12:00:00-04:00</published><updated>2013-08-27T12:00:00-04:00</updated><author><name>Thomas Wiecki</name></author><id>tag:twiecki.io,2013-08-27:/blog/2013/08/27/bayesian-glms-2/</id><content type="html">&lt;p&gt;{% notebook GLM-blog-robust.ipynb %}
~&lt;/p&gt;</content><category term="misc"></category><category term="bayesian statistics"></category></entry><entry><title>The Inference Button: Bayesian GLMs made easy with PyMC3</title><link href="https://twiecki.io/blog/2013/08/12/bayesian-glms-1/" rel="alternate"></link><published>2013-08-12T07:00:00-04:00</published><updated>2013-08-12T07:00:00-04:00</updated><author><name>Thomas Wiecki</name></author><id>tag:twiecki.io,2013-08-12:/blog/2013/08/12/bayesian-glms-1/</id><content type="html">&lt;p&gt;{% notebook GLM-blog-linear.ipynb %}
~&lt;/p&gt;</content><category term="misc"></category><category term="bayesian statistics"></category></entry></feed>